/*
package game.visual;
import org.deeplearning4j.rl4j.learning.sync.qlearning.QLearning.QLConfiguration;
import org.deeplearning4j.rl4j.learning.sync.qlearning.QLearningDiscreteDense;
import org.deeplearning4j.rl4j.network.dqn.DQNFactoryStdDense;
import org.deeplearning4j.rl4j.policy.DQNPolicy;
import org.deeplearning4j.rl4j.util.DataManager;


public class Neural_Network_V2 {
    QLConfiguration qlConf = new QLConfiguration(
            123,               // Seed aléatoire pour la reproductibilité
            0,                 // Nombre maximal d'étapes par épisode (0 pour illimité)
            1500,              // Nombre total d'épisodes d'entraînement
            100,               // Taille maximale de la mémoire de rejeu
            32,                // Taille du lot pour l'entraînement du réseau
            10,                // Fréquence de mise à jour du réseau cible
            10,                // Nombre d'étapes avant de commencer l'apprentissage
            0.1,               // Facteur de récompense (reward factor)
            0.99,              // Facteur de remise (gamma)
            0.01,              // Bornage des erreurs (error clamp)
            (float) 0.1,               // Taux d'exploration minimal (min epsilon)
            1000,              // Nombre d'étapes pour atteindre le taux d'exploration minimal
            false              // Utilisation de Double DQN
    );

}
*/
